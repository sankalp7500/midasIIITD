{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing Libraries that are useful such as json for dumping json data ,csv for better visualisation ,tweepy for extraction data from twitter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tweepy\n",
    "import json\n",
    "import csv\n",
    "import jsonpickle\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating a class Extractor which authorize from twitter app with the help of secret keys and tokens and get_tweets for data extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Extractor:\n",
    "    def __init__(self,account_name):\n",
    "        self.account_name = account_name\n",
    "\n",
    "    def authorize(self):\n",
    "        #Using OAuth interface to authorize our app to access Twitter\n",
    "        consumer_key = \"\"\n",
    "        consumer_secret = \"\"\n",
    "        access_token = \"\"\n",
    "        access_secret = \"\"\n",
    "        auth = tweepy.OAuthHandler(consumer_key, consumer_secret)\n",
    "        auth.set_access_token(access_token, access_secret)\n",
    "        \n",
    "        #Calling API\n",
    "        self.api = tweepy.API(auth)\n",
    "\n",
    "    def get_tweets(self):\n",
    "        tweets = self.api.user_timeline(screen_name=self.account_name,count=200)\n",
    "        data = jsonpickle.encode(tweets)\n",
    "\n",
    "        with open('tweets.json', 'w') as outfile:\n",
    "            json.dump(data, outfile)\n",
    "\n",
    "        config = json.loads(open('tweets.json').read())\n",
    "\n",
    "        tweets_for_csv = []\n",
    "        date_for_csv = []\n",
    "        fav_for_csv = []\n",
    "        media_for_csv = []\n",
    "        retweets_for_csv = []\n",
    "        \n",
    "\n",
    "        for tweet in tweets:\n",
    "            tweets_for_csv.append(tweet.text)\n",
    "            date_for_csv.append(tweet.created_at)\n",
    "            fav_for_csv.append(tweet.favorite_count)\n",
    "            media_for_csv.append(tweet.entities)\n",
    "            retweets_for_csv.append(tweet.retweet_count)\n",
    "  \n",
    "        # Empty Array \n",
    "        tmp1 = []\n",
    "        tmp2 = []\n",
    "        tmp3 = []\n",
    "        tmp4 = []\n",
    "        tmp5 = []\n",
    "        for j in tweets_for_csv: \n",
    "            # Appending tweets to the empty array tmp1 \n",
    "            tmp1.append(j)\n",
    "\n",
    "        for j in date_for_csv: \n",
    "            # Appending date and time to the empty array tmp2 \n",
    "            tmp2.append(j)\n",
    "\n",
    "        for j in fav_for_csv: \n",
    "            # Appending fav to the empty array tmp3 \n",
    "            tmp3.append(j)\n",
    "\n",
    "        for j in media_for_csv: \n",
    "            # Appending media to the empty array tmp4\n",
    "            try:\n",
    "                l = len(j['media'])\n",
    "                tmp4.append(1)\n",
    "            except:\n",
    "                tmp4.append(0)\n",
    "\n",
    "        for j in retweets_for_csv: \n",
    "            # Appending retweets to the empty array tmp5 \n",
    "            tmp5.append(j)\n",
    "\n",
    "        submission = pd.DataFrame({\"tweets_text\": tmp1, \"date\": tmp2,\"favourite_count\": tmp3, \"no_of_images\": tmp4,\"retweet_count\": tmp5})\n",
    "        print(submission)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Authorize function contains keys and tokens and with the help of which we may get permission for data mining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "get_tweets method first with the help of user_timeline extract data the with the json pickle dumps it into a json file.The we again read the data from that json file and append all data into multiple arrays.The we create a dataframe to combine all arrays and present them as indiviual columns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling the functions of the class Extractor with the help of username of twitter handle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                           tweets_text                date  \\\n",
      "0    RT @multimediaeval: Announcing the 2019 MediaE... 2019-04-08 19:38:09   \n",
      "1    Many Congratulations to @midasIIITD student, S... 2019-04-08 07:08:12   \n",
      "2    @midasIIITD thanks all students who have appea... 2019-04-08 03:27:42   \n",
      "3    @himanchalchandr Meanwhile, complete CV/NLP ta... 2019-04-07 14:17:29   \n",
      "4    @sayangdipto123 Submit as per the guideline ag... 2019-04-07 14:17:09   \n",
      "5    We request all students whose interview are sc... 2019-04-07 11:43:24   \n",
      "6    Other queries: \"none of the Tweeter Apis give ... 2019-04-07 06:55:19   \n",
      "7    Other queries: \"do we have to make two differe... 2019-04-07 06:53:38   \n",
      "8    Other queries: \"If using Twitter api, it does ... 2019-04-07 05:32:27   \n",
      "9    Response to some queries asked by students on ... 2019-04-07 05:29:40   \n",
      "10   RT @kdnuggets: Top 8 #Free Must-Read #Books on... 2019-04-06 17:11:29   \n",
      "11   @nupur_baghel @PennDATS Congratulation @nupur_... 2019-04-06 16:43:27   \n",
      "12   We have emailed the task details to all candid... 2019-04-05 16:08:37   \n",
      "13   RT @rfpvjr: Our NAACL paper on polarization in... 2019-04-05 04:05:11   \n",
      "14   RT @kdnuggets: Effective Transfer Learning For... 2019-04-05 04:04:43   \n",
      "15   RT @stanfordnlp: Whatâ€™s new in @Stanford CS224... 2019-04-03 18:31:53   \n",
      "16   RT @DeepMindAI: Today we're releasing a large-... 2019-04-03 17:04:32   \n",
      "17   RT @ylecun: Congratulations Jitendra Malik !\\n... 2019-04-03 09:03:40   \n",
      "18   RT @IIITDelhi: Another chance to take admissio... 2019-04-03 07:46:02   \n",
      "19   Dear @midasIIITD internship candidates who hav... 2019-04-02 04:20:13   \n",
      "20   Looking forward to your paper submission to @I... 2019-04-02 02:44:54   \n",
      "21   RT @ngrams: Reproducibility in multimedia rese... 2019-04-02 02:35:44   \n",
      "22   Online application for https://t.co/DJFDrQsHZP... 2019-04-01 06:53:08   \n",
      "23   RT @ACMMM19: A final reminder of the Reproduci... 2019-03-31 10:21:24   \n",
      "24   RT @isarth23: Thanks for the support and help ... 2019-03-29 19:43:24   \n",
      "25   Since SemEval-2019 will be held June 6-7, 2019... 2019-03-29 17:16:40   \n",
      "26   +@aggarwal_kartik.\\nCongrats! Wish you many mo... 2019-03-29 17:04:30   \n",
      "27   RT @aggarwal_kartik: Our work (@midasIIITD ) a... 2019-03-29 17:03:29   \n",
      "28   Congratulations! @midasIIITD team, @isarth23 @... 2019-03-29 17:02:24   \n",
      "29   @EEMLcommunity @radamihalcea too many deadline... 2019-03-29 05:35:22   \n",
      "..                                                 ...                 ...   \n",
      "170  Since 1993, ACM Multimedia has been bringing t... 2019-01-06 04:09:54   \n",
      "171  @ACMMM19 is structured around four themes (alp... 2019-01-06 04:07:29   \n",
      "172  ACM Multimedia 2019: First Call for Papers/Cal... 2019-01-06 04:05:38   \n",
      "173  RT @ACMMM19: Happy new year! 2019 started with... 2019-01-03 06:14:05   \n",
      "174  You may follow @midasIIITD updates through fol... 2019-01-01 17:21:49   \n",
      "175  Today, @midasIIITD completed one year at @IIIT... 2019-01-01 17:17:56   \n",
      "176  RT @debanjanbhucs: Happy anniversary to @midas... 2019-01-01 16:14:24   \n",
      "177  Two Research Assistant (RA) positions are stil... 2018-12-27 10:28:31   \n",
      "178  Positions for two Research Assistants (RA) are... 2018-12-27 09:59:06   \n",
      "179  RT @TCoolsIT: I wrote a blogpost on CoreNLP hi... 2018-12-27 02:33:58   \n",
      "180  @RatnRajiv delivered a research talk at the Ce... 2018-12-26 09:41:04   \n",
      "181  @midasIIITD look forward to work together. htt... 2018-12-25 04:15:15   \n",
      "182  @the_dhumketu @punnibhai @Hitkul_, FYI. https:... 2018-12-25 03:00:21   \n",
      "183  RT @iiit_hyderabad: KCIS Distinguished Lecture... 2018-12-24 07:39:21   \n",
      "184  RT @StanfordAILab: Our latest blog post is out... 2018-12-21 03:50:39   \n",
      "185  RT @medialab: In two new papers, @PratikShahPh... 2018-12-20 18:05:35   \n",
      "186  RT @RatnRajiv: Wonderful get together with our... 2018-12-20 06:14:52   \n",
      "187  @ACMMM19 is the premier international conferen... 2018-12-19 02:16:31   \n",
      "188  Feel free to contact us if you have any query ... 2018-12-18 14:42:27   \n",
      "189  @NilayShri @RatnRajiv @ACMMM19 Looking forward... 2018-12-18 14:27:12   \n",
      "190  @midasIIITD head, Dr. @RatnRajiv appointed as ... 2018-12-18 14:15:32   \n",
      "191  @ACMMM19 @the_dhumketu fill the volunteer form... 2018-12-18 12:34:10   \n",
      "192  RT @ACMMM19: New in 2019! Volunteer to serve a... 2018-12-18 12:33:42   \n",
      "193  RT @IIITDelhi: Congratulations! authors Yaman ... 2018-12-18 04:38:35   \n",
      "194  Best wishes to @midasIIITD students @_himanshu... 2018-12-17 16:57:00   \n",
      "195  RT @MIT: Computer model could improve human-ma... 2018-12-17 15:05:44   \n",
      "196  RT @kdnuggets: Great @Wired interview with Geo... 2018-12-17 14:37:00   \n",
      "197  RT @harvardmed: A defective metabolic pathway ... 2018-12-17 14:05:47   \n",
      "198  RT @stanfordnlp: Thread on (neural) reasoning.... 2018-12-16 15:01:20   \n",
      "199  Congratulations! Yaman @the_dhumketu, Rohit, S... 2018-12-12 03:56:19   \n",
      "\n",
      "     favourite_count  no_of_images  retweet_count  \n",
      "0                  0             0             15  \n",
      "1                 13             0              2  \n",
      "2                  4             0              0  \n",
      "3                  0             0              0  \n",
      "4                  0             0              0  \n",
      "5                  1             0              1  \n",
      "6                  5             0              2  \n",
      "7                  4             0              1  \n",
      "8                  6             0              1  \n",
      "9                  7             0              1  \n",
      "10                 0             0              2  \n",
      "11                18             0              3  \n",
      "12                11             0              1  \n",
      "13                 0             0             16  \n",
      "14                 0             1             11  \n",
      "15                 0             0             59  \n",
      "16                 0             0            870  \n",
      "17                 0             0             16  \n",
      "18                 0             0              4  \n",
      "19                 8             0              1  \n",
      "20                 5             0              1  \n",
      "21                 0             0              7  \n",
      "22                 7             0              2  \n",
      "23                 0             0             10  \n",
      "24                 0             0              2  \n",
      "25                 9             0              1  \n",
      "26                 2             0              0  \n",
      "27                 0             0              1  \n",
      "28                 9             0              1  \n",
      "29                 0             0              0  \n",
      "..               ...           ...            ...  \n",
      "170                5             0              1  \n",
      "171                1             0              0  \n",
      "172                4             0              2  \n",
      "173                0             0              2  \n",
      "174                4             0              2  \n",
      "175               14             0              5  \n",
      "176                0             0              4  \n",
      "177                2             0              1  \n",
      "178                5             0              3  \n",
      "179                0             0             12  \n",
      "180               14             0              2  \n",
      "181                1             0              0  \n",
      "182                2             0              1  \n",
      "183                0             0             11  \n",
      "184                0             0             32  \n",
      "185                0             0              8  \n",
      "186                0             0              3  \n",
      "187                2             0              3  \n",
      "188                3             0              2  \n",
      "189                3             0              0  \n",
      "190               12             0              5  \n",
      "191                1             0              0  \n",
      "192                0             0              7  \n",
      "193                0             0              3  \n",
      "194               14             0              4  \n",
      "195                0             0             20  \n",
      "196                0             0             20  \n",
      "197                0             0            167  \n",
      "198                0             0             10  \n",
      "199               16             0              3  \n",
      "\n",
      "[200 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "obj1 = Extractor('midasIIITD')\n",
    "obj1.authorize()\n",
    "obj1.get_tweets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
